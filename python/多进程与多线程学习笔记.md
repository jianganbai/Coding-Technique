# 多进程与多线程学习笔记

- python解释器默认线程安全
  - GIL (Global Interpreter Lock)锁：**解释器同时只能运行1个线程**（无论多少个核）
    - Python解释器进程有1个主线程，和多个用户程序线程
    - 等I/O时线程阻塞，可切换线程
    - 多进程可绕过GIL锁，或者使用CPython编译
  - **多进程适合计算密集任务**
  - **多线程适合I/O密集任务**

## 多进程Multiprocessing

- 基于multiprocessing库

  - ```python
    import multiprocessing as mp
    
    def function1(id):  # 这里是子进程
        print(f'id {id}')
    
    def run__process():  # 这里是主进程
        process = [mp.Process(target=function1, args=(1,)),  # args是传入的参数
                   mp.Process(target=function1, args=(2,)), ]
        [p.start() for p in process]  # 开启了两个进程
        [p.join() for p in process]   # 等待两个进程依次结束
    
    # run__process()  # 主线程不建议写在 if外部。由于这里的例子很简单，你强行这么做可能不会报错
    if __name__ =='__main__':
        run__process()  # 正确做法：主线程只能写在 if内部
    ```


### 创建进程

- 主进程要写在`if __name__ == '__main__'`里面

- 创建子进程

  - fork: 复制主进程全部数据给子进程

  - spawn: 仅复制主进程必要数据给子进程

    - ```python
      mp.set_start_method('fork')  # Linux默认
      mp.set_start_method('spawn')  # Windows默认
      ```

#### Process

- 创建进程的最基本方法：适合进程数较少

  - ```python
    from multiprocessing import Process
    
    # Process(group, target, name, args, kwargs)
    # group=None, target是子进程执行的函数名，name是子进程名字
    # args以元组方式传入参数，kwargs以字典方式传入参数
    def run_proc(name):
        print('Run child process %s (%s)...' % (name, os.getpid()))
    
    if __name__=='__main__':
        print('Parent process %s.' % os.getpid())
        p = Process(target=run_proc, args=('test',))
        print('Child process will start.')
        p.start()
        p.join()
    print('Child process end.')
    ```

  - Process对象属性值：`p.name`：进程名字；`p.pid`：进程pid

  - Process对象内置函数

    - `p.start()`：启动进程；`p.terminate()`：终止进程；
    - `p.is_alive()`：判断进程是否在运行；`p.join(timeout)`：主进程等待子进程p执行完毕

#### 进程池Pool

- ```python
  from multiprocessing import Pool
  
  def test(i):
      print(i)
  
  if __name__ == "__main__":
      lists = range(100)
      p = Pool(8)
      job = p.map(test, lists)  # 创建完全部进程后，再开始子进程，然后主进程阻塞
      p.close()  # Pool不再接受新任务
      p.join()  # 主进程等所有任务完成然后销毁Pool，再进行后续
      
      p = Pool(8)
      for a in lists:
          p.apply_async(test, a)
      p.close()
      p.join()
  ```

- **创建开始**

  - `Pool.apply`：**创建子进程就开始**
    - `p.apply(func, args, kwargs)`：子进程开始后，主进程阻塞，没法用
      - `args=(a, b)`或`kwargs={'a': a, 'b': b}`
      - 每次只传入1个任务的参数

    - `p.apply_async(func, args, kwargs)`：子进程开始后立即返回1个对象，主进程因而不阻塞继续

  - `Pool.map`：**创建完全部子进程，再统一开始**
    - `p.map(func, iterable)`
      - iterable为所有任务的参数
      - func只能有1个参数，否则需要用starmap
      - **主进程阻塞**，等待全部子进程完成任务

    - `p.map_async(func, iterable)`：**主进程不阻塞**，仅当`p.wait()`时才会阻塞

  - `Pool.imp`：完成就返回结果，map是等所有结果都返回后才返回
    - `p.imap(func, iterable)`
    - `p.imap_unordered(func, iterbale)`：imap保留输入输出顺序，imap_unordered不保留输入输出顺序
  - `Pool.starmap`
    - `p.starmap(func, iterable)`：map只能传1个参数，star_map能传多个参数
    - `p.starmap_async(func, iterable)`：主进程不阻塞


- 返回值

  - async方法

    - ```python
      def test(a):
          return a
      
      p = mp.Pool(5)
      res = []
      for a in range(20):
          res.append(p.apply_async(test, (a,)))  # 不阻塞，所以不能直接调用.get
      p.close()
      p.join()
      for r in res:
          r.get()  # 提取返回值
      ```

    - ```python
      # 回调函数：子进程执行完任务后调用，参数为返回值
      def log(v):
          callback_res.append(v)
      
      for a in range(20):
          pool.apply_async(test, (a,), callback=log)
      pool.close()
      pool.join()
      print(callback_res)
      ```

    - ```shell
      # 监控任务完成情况
      pbar = tqdm(total=len(data))
      update=lambda *args: pbar.update()
      for a in range(20):
      	pool.apply_async(test, (a,), callback=update)
      ```

- 其它
  - `list(tqdm.tqdm(p.imap(func, range(10)), total=10))`：imap返回的iter给tqdm，list()启动迭代
  - `job.ready()`：若所有任务完成，则返回True
  - `mp.cpu_count()`：获取CPU核数，一般总进程数 <= 核数

### 进程间通信

- 方法：进程池Pool、管道Pipe、队列Queue、共享内存Manager

#### 管道Pipe

- Pipe

  - ```python
    import time
    from multiprocessing import Process, Pipe
    
    def func_pipe1(conn, p_id):
        print(p_id, flush=True)
        time.sleep(0.1)
        conn.send(f'{p_id}_send')
        print(p_id, 'send')
        time.sleep(0.1)
        rec = conn.recv()
        print(p_id, 'recv', rec, flush=True)
    
    def func_pipe2(conn, p_id):
        print(p_id, flush=True)
        time.sleep(0.1)
        conn.send(p_id)
        print(p_id, 'send', flush=True)
        time.sleep(0.1)
        rec = conn.recv()
        print(p_id, 'recv', rec, flush=True)
    
    def run__pipe():
        conn1, conn2 = Pipe(duplex=True)  # conn1和conn2分别是管道的2端, duplex为是否双向
        process = [Process(target=func_pipe1, args=(conn1, 'I1')),
                   Process(target=func_pipe2, args=(conn2, 'I2')),
                   Process(target=func_pipe2, args=(conn2, 'I3')), ]
        [p.start() for p in process]  # 主进程开启所有子进程
        print('| Main', 'send')
        conn1.send(None)
        print('| Main', conn2.recv())
        [p.join() for p in process]  # 主进程等待所有子进程结束后再继续
    
    if __name__ =='__main__':
        run__pipe()
    ```

  - 双向管道，`conn.send()`发送，`conn.recv()`取出

    - 从一端送入，仅能从另一端取出
    - 送进去的都经过深拷贝

  - `conn2.poll(1)`：等1秒再接收

#### 队列Queue

- 队列Queue：主进程/子进程都能访问到

  - ```python
    from multiprocessing import Process, Queue
    
    def func1(i):
        time.sleep(1)
        print(f'args {i}')
    
    def run__queue():
        queue = Queue(maxsize=4)
        queue.put(True)  # blocked=True：若队列满，等待timeout时间后抛出异常
        queue.put([0, None, object])  # you can put deepcopy thing
        queue.qsize()  # the length of queue
        print(queue.get())  # 读取第1个并删除
        print(queue.get())  # First In First Out
        queue.qsize()  # the length of queue
    
        process = [Process(target=func1, args=(queue,)),  # Queue当作参数传入子进程
                   Process(target=func1, args=(queue,)), ]
        [p.start() for p in process]
        [p.join() for p in process]
    
    if __name__ =='__main__':
        run__queue()
    ```

#### 共享内存Manager

- 共享内存Manager
  - 解释器维护共享内存，不深拷贝
  - 仅能使用multiprocessing的数据结构：list, dict, Lock, Queue, Value, Array
    - list, dict内部的数据类型可以有多种

##### Dict, List

- ```python
  import os
  import numpy as np
  import multiprocessing
  
  data = np.arange(20)
  # 默认data地址共享
  # 也可将data, worker和main都包在1个class中
  # 若将data作为参数，则需要对data进行深拷贝
  
  def worker(idx, return_dict, return_list):
      print(f'memory for data in process {os.getpid()}: {id(data)}', flush=True)
      return_dict[idx] = data[idx] // 2
      return_list.append(data[idx] // 2)
  
  def main():
      manager = multiprocessing.Manager()
      return_dict = manager.dict()
      return_list = manager.list()
      task_list = [(idx, return_dict, return_list) for idx in range(data.shape[0])]
      pool = multiprocessing.Pool(5)
      pool.starmap(worker, task_list)
      pool.close()  # 不再向pool提交任务
      pool.join()  # 主进程等待所有子进程结束
      print(return_dict)  # 使用dict(return_dict)转化为普通dict
      print(return_list)  # 使用dict(return_list)转化为普通list
  
  if __name__ == '__main__':
      main()
  ```

##### Value, Array

- ```python
  import multiprocessing as mp
  
  def f(n, a):
      n.value = 3.14
      a[0] = 5
  
  if __name__ == '__main__':
      num = mp.Value('d', 0.0)
      arr = mp.Array('i', range(10))
      p = mp.Process(target=f, args=(num, arr))  # 共享变量作为参数传入
      p.start()
      p.join()
      print(num.value)
      print(arr[:])
  ```

  - `Array(typecode_or_type, size_or_initializer, lock=True)`
    - `typecode_or_type`：数据类型，'i'为int，'f'为float，'d'为double
    - `size_or_initializer`：数组大小（仅支持一维数据），或者初始化数组的序列（数组长度=序列长度）

  - `Value(typecode_or_type, args, lock=True)`：默认有锁保护

- 以Numpy数组作为共享内存

  - ```python
    import ctypes as c
    import numpy as np
    import multiprocessing as mp
    
    n, m = 2, 3
    mp_arr = mp.Array(c.c_double, n*m) # shared, can be used from multiple processes
    # then in each new process create a new numpy array using:
    arr = np.frombuffer(mp_arr.get_obj()) # mp_arr and arr share the same memory
    # make it two-dimensional
    b = arr.reshape((n,m)) # b and arr share the same memory
    # 之后将b作为参数传入子进程
    ```

### 进程间同步

#### 互斥锁Lock

- 必须在全局定义，当作参数传给子进程

  - ```python
    from multiprocessing import Process, Lock
    
    def l(lock, num):
        lock.acquire()
        print("Hello Num: %s" % (num), flush=True)
        lock.release()
    
    if __name__ == '__main__':
        lock = Lock()  # 这个一定要定义为全局
        for num in range(20):
            Process(target=l, args=(lock, num)).start()
    ```

#### 信号量Semaphore

- 带计数功能的Lock

  - ```python
    from multiprocessing import Process, Semaphore
    import time, random
    
    def go_wc(sem, user):
        sem.acquire()
        print('%s 占到一个茅坑' % user, flush=True)
        time.sleep(random.randint(0, 3))
        sem.release()
        print(user, 'OK')
    
    if __name__ == '__main__':
        sem = Semaphore(2)
        p_l = []
        for i in range(5):
            p = Process(target=go_wc, args=(sem, 'user%s' % i,))
            p.start()
            p_l.append(p)
        for i in p_l:
            i.join()
    ```

## ProcessPoolExecutor

- 来源：`from concurrent.futures import ProcessPoolExecutor`

  - 相比multiprocessing封装更高层，可使用多进程和多线程且调用方式差不多

  - ```python
    
    import numpy as np
    import multiprocessing
    from concurrent.futures.process import ProcessPoolExecutor
    
    NUM_WORKERS = multiprocessing.cpu_count()
    np.random.seed(42)
    ARRAY_SIZE = int(2e8)
    data = np.random.random(ARRAY_SIZE)  # 任务：计算数组的sum
    
    def np_sum_global(start, stop):
        return np.sum(data[start:stop])
    
    def benchmark():
        chunk_size = int(ARRAY_SIZE / NUM_WORKERS)
        futures = []
        ts = time.time_ns()
        with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
            for i in range(0, NUM_WORKERS):
                start = i + chunk_size if i == 0 else 0
                futures.append(executor.submit(np_sum_global, start, i + chunk_size))
        futures, _ = concurrent.futures.wait(futures)
        return (time.time_ns() - start_time) / 1_000_000
    ```

## 多线程

- Python有GIL，同时只能有1个线程处于执行状态
  - 适合I/O或者爬虫这种长时间阻塞的
  - 不适合计算密集型 => 应该用多进程

### 基于threading

- ```python
  import threading
  
  lock = threading.Lock()
  
  class Thread(threading.Thread):
  	def __init__(self, thredID):
  		threading.Thread.__init__(self)
  		self.threadID = threadID
  	def run(self)
      	# calculate
          lock.acquire()
          # 修改临界区
          lock.release()
  
  if __name__ == '__main__':
      thread = []
      for i in range(thread_num):
          thread.append(Thread(i + 1))
      for i in range(thread_num):
          thread[i].start()
      for i in range(thread_num):
          thread[i].join()
  ```
  
- 
