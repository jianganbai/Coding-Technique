# 多进程与多线程学习笔记

- python解释器默认线程安全
  - GIL (Global Interpreter Lock)锁：**解释器同时只能运行1个线程**（无论多少个核）
    - Python解释器进程有1个主线程，和多个用户程序线程
    - 等I/O时线程阻塞，可切换线程
    - 多进程可绕过GIL锁，或者使用CPython编译
  - **多进程适合计算密集任务**
  - **多线程适合I/O密集任务**

## 多进程Multiprocessing

- 基于multiprocessing库
  
  - ```python
    import multiprocessing as mp
    
    def function1(id):  # 这里是子进程
        print(f'id {id}')
    
    def run__process():  # 这里是主进程
        process = [mp.Process(target=function1, args=(1,)),  # args是传入的参数
                   mp.Process(target=function1, args=(2,)), ]
        [p.start() for p in process]  # 开启了两个进程
        [p.join() for p in process]   # 等待两个进程依次结束
    
    # run__process()  # 主线程不建议写在 if外部。由于这里的例子很简单，你强行这么做可能不会报错
    if __name__ =='__main__':
        run__process()  # 正确做法：主线程只能写在 if内部
    ```

### 创建进程

- 主进程要写在`if __name__ == '__main__'`里面

- 创建子进程
  
  - fork: 复制主进程全部数据给子进程
    
    - 更快
  
  - spawn: 仅复制主进程必要数据给子进程，子进程重新创建资源
    
    - 与CUDA相关，必须使用spawn
    
    - ```python
      mp.set_start_method('fork')  # Linux默认
      mp.set_start_method('spawn')  # Windows默认
      ```

#### Process

- 创建进程的最基本方法：适合进程数较少
  
  - ```python
    from multiprocessing import Process
    
    # Process(group, target, name, args, kwargs)
    # group=None, target是子进程执行的函数名，name是子进程名字
    # args以元组方式传入参数，kwargs以字典方式传入参数
    def run_proc(name):
        print('Run child process %s (%s)...' % (name, os.getpid()))
    
    if __name__=='__main__':
        print('Parent process %s.' % os.getpid())
        p = Process(target=run_proc, args=('test',))
        print('Child process will start.')
        p.start()
        p.join()
    print('Child process end.')
    ```
  
  - Process对象属性值：`p.name`：进程名字；`p.pid`：进程pid
  
  - Process对象内置函数
    
    - `p.start()`：启动进程；`p.terminate()`：终止进程；
    - `p.is_alive()`：判断进程是否在运行；`p.join(timeout)`：主进程等待子进程p执行完毕

- Process需要手动管理子进程的任务分配、生灭，Pool能自动分配任务

#### 进程池Pool

- ```python
  import multiprocessing as mp
  
  def test(i):
      print(i)
  
  if __name__ == "__main__":
      lists = range(100)
      p = mp.Pool(8)
      job = p.map(test, lists)  # 创建完全部进程后，再开始子进程，然后主进程阻塞
      p.close()  # Pool不再接受新任务
      p.join()  # 主进程等所有任务完成然后销毁Pool，再进行后续
  
      p = Pool(8)
      for a in lists:  # 执行完任务，就自动取下一个
          p.apply_async(test, a)
      p.close()
      p.join()
  ```

- **创建开始**
  
  - `Pool.apply`：**创建子进程就开始**
    
    - `p.apply(func, args, kwargs)`：子进程开始后，主进程阻塞，没法用
      
      - `args=(a, b)`或`kwargs={'a': a, 'b': b}`
      - 每次只传入1个任务的参数
    
    - `p.apply_async(func, args, kwargs)`：子进程开始后立即返回1个对象，主进程因而不阻塞继续
      
      - ```python
        res = []
        for i in range(10):
            res.append(p.apply_async(func, (i,)))
        p.close()
        p.join()
        for i in range(10):
          print(res[i].get())  # .get()会阻塞主进程，直到返回结果，应等所有子进程都结束后统一调用
          # 可返回picklable的数据类型
        ```
  
  - `Pool.map`：**创建完全部子进程，再统一开始**
    
    - `p.map(func, iterable)`
      
      - iterable为所有任务的参数
      - func只能有1个参数（可以将多个参数打包成元组），否则需要用starmap
      - **主进程阻塞**，等待全部子进程完成任务
    
    - `p.map_async(func, iterable)`：**主进程不阻塞**，仅当`p.wait()`时才会阻塞
  
  - `Pool.imap`：完成就返回结果，map是等所有结果都返回后才返回
    
    - `p.imap(func, iterable)`
    - `p.imap_unordered(func, iterbale)`：imap保留输入输出顺序，imap_unordered不保留输入输出顺序
  
  - `Pool.starmap`
    
    - `p.starmap(func, iterable)`：map只能传1个参数，star_map能传多个参数
      - iterable的每个元素为1个元组
    - `p.starmap_async(func, iterable)`：主进程不阻塞

- 返回值
  
  - async方法
    
    - ```python
      def test(a):
          return a
      
      p = mp.Pool(5)
      res = []
      for a in range(20):
          res.append(p.apply_async(test, (a,)))  # 不阻塞，所以不能直接调用.get
      p.close()
      p.join()
      for r in res:
          r.get()  # 提取返回值
      ```
    
    - ```python
      # 回调函数：子进程执行完任务后调用，参数为返回值
      def log(v):
          callback_res.append(v)
      
      for a in range(20):
          p.apply_async(test, (a,), callback=log)
      p.close()
      p.join()
      print(callback_res)
      ```
    
    - ```shell
      # 监控任务完成情况
      pbar = tqdm(total=len(data))
      for a in range(20):
          pool.apply_async(
              func=test,
              args=(a,),
              callback=lambda *args: pbar.update(),  # args是test函数的输出
              error_callback=lambda error: print(error)
          )
      ```

- 子进程初始化
  
  - ```python
    def init_func(a):
        global b  # b在子进程内部为global，故worker函数能直接访问到b
        b = a
    def worker(c):
        print(b + c)
    # 每个子进程调用init_func初始化，初始化参数固定
    P = mp.Pool(num_proc, initializer=init_func, initargs=(a,))
    for i in range(10):
        P.apply_async(worker, (i,))
    ```
  
  - ```python
    class worker:
        def __init__(self, args):
            # 初始化参数
        def work(self):
            # 处理
    P = mp.Pool(num_proc)
    worker_list = [worker(args) for _ in range(num_worker)]
    for w in worker_list:
        P.apply_async(w.work)
    # 可初始化不同子进程，但不能给不同子进程传参
    # 若需要传参，可通过queue
    ```

- 要求
  
  - `mp.Pool`创建的是守护进程（Daemon Process），守护进程内不能创建新的子进程

- 其它
  
  - `list(tqdm.tqdm(p.imap(func, range(10)), total=10))`：imap返回的iter给tqdm，list()启动迭代
  - `job.ready()`：若所有任务完成，则返回True
  - `mp.cpu_count()`：获取CPU核数，一般总进程数 <= 核数

### 进程间通信

|               | 优点                    | 缺点                        | 适用场景              |
| ------------- | --------------------- | ------------------------- | ----------------- |
| Pipe          | 延迟低；原子操作              | 仅能用于2个进程；需要pickle序列化/反序列化 | 低延迟小数据量           |
| Queue         | 多进程通信；包含互斥锁           | 比Pipe速度略低                 | 多进程生产者-消费者        |
| Manager       | 支持多种数据；共享内存；包含互斥锁     | 速度慢；有可能死锁                 | 多进程共同修改           |
| shared_memory | 直接操作内存，省去pickle；零拷贝共享 | 仅基本数据类型；需手动管理同步；需手动释放内存   | 共享大型数据集（如numpy数组） |

#### 管道Pipe

- `Pipe`：2个进程点到点通信
  
  - ```python
    from multiprocessing import Process, Pipe
    
    def func_pipe1(conn, p_id):
        conn.send(f'{p_id}_send')
        rec = conn.recv()  # I2
    
    def func_pipe2(conn, p_id):
        conn.send(p_id)nn
        rec = conn.recv()  # I1_send
    
    conn1, conn2 = Pipe(duplex=True)  # conn1和conn2分别是管道的2端, duplex为是否双向
    process = [Process(target=func_pipe1, args=(conn1, 'I1')),
               Process(target=func_pipe2, args=(conn2, 'I2'))]
    [p.start() for p in process]  # 主进程开启所有子进程
    [p.join() for p in process]  # 主进程等待所有子进程结束后再继续
    ```
  
  - ```python
    # conn.recv默认阻塞接受者
    if conn.poll(0.5):  # 等待0.5秒
        msg = parent_conn.recv()
    else:
        # do other work
    ```

- 通过管道传输的数据会进行pickle序列化和反序列化，发送和接收的数据内存地址不同

#### 队列Queue

- 队列`Queue`：主进程/子进程都能访问到
  
  - ```python
    from multiprocessing import Process, Queue
    
    def func1(i):
        time.sleep(1)
        print(f'args {i}')
    
    queue = Queue(maxsize=4)
    queue.put(True)  # blocked=True：若队列满，等待timeout时间后抛出异常
    queue.put([0, None, object])  # you can put deepcopy thing
    queue.qsize()  # the length of queue
    print(queue.get())  # 读取第1个并删除
    print(queue.get())  # First In First Out
    queue.qsize()  # the length of queue
    
    process = [Process(target=func1, args=(queue,)),  # Queue当作参数传入子进程
               Process(target=func1, args=(queue,)), ]
    [p.start() for p in process]
    [p.join() for p in process]
    ```

- 只有`mp.Process`可使用`mp.Queue`，`mp.Pool`应使用`Manager.Queue`
  
  - `Manager.Queue`用法与`mp.Queue`一致，需要先实例化manager，再定义Queue

#### 共享内存Manager

- 共享内存Manager
  - 解释器维护共享内存，不深拷贝
  - 仅能使用multiprocessing的数据结构：list, dict, Lock, Queue, Value, Array
    - list, dict内部的数据类型可以有多种

##### Dict, List

- ```python
  import os
  import numpy as np
  import multiprocessing
  
  data = np.arange(20)
  # 默认data地址共享
  # 也可将data, worker和main都包在1个class中
  # 若将data作为参数，则需要对data进行深拷贝
  
  def worker(idx, return_dict, return_list):
      print(f'memory for data in process {os.getpid()}: {id(data)}', flush=True)
      return_dict[idx] = data[idx] // 2
      return_list.append(data[idx] // 2)
  
  def main():
      manager = multiprocessing.Manager()
      return_dict = manager.dict()
      return_list = manager.list()
      task_list = [(idx, return_dict, return_list) for idx in range(data.shape[0])]
      pool = multiprocessing.Pool(5)
      pool.starmap(worker, task_list)
      pool.close()  # 不再向pool提交任务
      pool.join()  # 主进程等待所有子进程结束
      print(return_dict)  # 使用dict(return_dict)转化为普通dict
      print(return_list)  # 使用list(return_list)转化为普通list
  
  if __name__ == '__main__':
      main()
  ```

- 操作Dict, List时，是否需要单独配合互斥锁？
  
  - 原子操作是安全的：`dict[a] = 1`, `list.append()`
  
  - 非原子操作需要加锁：`list[i] + =1`, `dict[key] += 1`

##### Value, Array

- 定义
  
  - `Array(typecode_or_type, size_or_initializer, lock=True)`
    
    - `typecode_or_type`：数据类型，'i'为int，'f'为float，'d'为double
    - `size_or_initializer`：数组大小（仅支持一维数据），或者初始化数组的序列（数组长度=序列长度）
    - `mp.Array`必须使用`mp.Process`，不能使用`mp.Pool`
  
  - `Value(typecode_or_type, args, lock=True)`：默认有锁保护

- 例子
  
  - ```python
    import multiprocessing as mp
    
    def f(n, a):
        n.value = 3.14
        a[0] = 5
    
    if __name__ == '__main__':
        num = mp.Value('d', 0.0)
        arr = mp.Array('i', range(10))
        p = mp.Process(target=f, args=(num, arr))  # 共享变量作为参数传入
        p.start()
        p.join()
        print(num.value)
        print(arr[:])
    ```
  
  - ```python
    import multiprocessing as mp
    
    num = mp.Value('d', 0.0)
    
    def func(i):
        with num.get_lock():
            num += i
    
    if __name__ == '__main__':
        p = mp.Pool(2)
        for i in range(10):
            p.apply_async(func, (i,))
    ```

- 以Numpy数组作为共享内存
  
  - ```python
    import ctypes as c
    import numpy as np
    import multiprocessing as mp
    
    n, m = 2, 3
    mp_arr = mp.Array(c.c_double, n*m) # shared, can be used from multiple processes
    # mp_arr and arr share the same memory
    arr = np.frombuffer(mp_arr.get_obj(), dtype=np.float32).reshape((n,m))
    arr[:] = data  # in-place assignment, no copy
    
    # in each sub-process, use mp_arr
    np_arr = np.frombuffer(mp_arr.get_obj(), dtype=np.float64)
    ```

#### 共享内存shared_memory

- 纯字节流
  
  - 写入
    
    - ```python
      from multiprocessing import shared_memory
      shm = shared_memory.SharedMemory(name='my_shared_memory', create=True, size=10)  # 创建一块共享内存，大小为10字节
      buffer = shm.buf  # 写入数据
      buffer[0:5] = b'Hello'  # 写入前5个字节
      shm.buf[5:9] = i.to_bytes(4, byteorder='little')
      # 其它进程读取，全部结束后
      shm.close()  # 结束访问。所有进程都需要调用
      shm.unlink()  # 删除内存。只有创建者需要调用
      ```
  
  - 读取
    
    - ```python
      existing_shm = shared_memory.SharedMemory(name='my_shared_memory')  # 连接到已存在的共享内存
      print(bytes(existing_shm.buf[0:5]))  # 输出: b'Hello'
      data = int.from_bytes(shm.buf[5:9], byteorder='little')
      existing_shm.close()
      ```

- 搭配`numpy`
  
  - ```python
    import numpy as np
    from multiprocessing import shared_memory, Process
    
    def worker(shm_name, shape, dtype):
        existing_shm = shared_memory.SharedMemory(name=shm_name)  # 访问现有共享内存
        np_array = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)
        np_array[0] = 100  # 修改数据（对其他进程立即可见）
        existing_shm.close()  # 子进程访问共享内存结束
    
    if __name__ == '__main__':
        # 创建共享内存并存储 NumPy 数组
        arr = np.array([1, 2, 3], dtype=np.int64)
        shm = shared_memory.SharedMemory(create=True, size=arr.nbytes)  # 创建共享内存
        shm_arr = np.ndarray(arr.shape, dtype=arr.dtype, buffer=shm.buf)  # 将共享内存包装为np数组
        np.copyto(shm_arr, arr)  # 拷贝数据到共享内存
    
        p = Process(target=worker, args=(shm.name, arr.shape, arr.dtype))
        p.start()
        p.join()
    
        print("Main process:", shm_arr)  # 输出: [100, 2, 3]
        shm.close()
        shm.unlink()  # 需要手动释放内存，仅共享内存创建者释放
    ```

- 共享内存需手动同步：写需要互斥、等所有进程读写完成后才可由创建者`unlink`
  
  - | 场景                       | 同步方式            | 内存管理                     | 适用情况     |
    | ------------------------ | --------------- | ------------------------ | -------- |
    | 强同步：生产者必须等消费者完成，才进行下一轮生产 | Event / Barrier | 生产者阻塞，等消费者通知后再启动并释放内存    | 必须等消费者完成 |
    | 异步流水线：生产和消费同时进行          | Queue + List    | 生产者定期查看Queue，收到消费者通知后再释放 | 高吞吐量     |

### 进程间同步

#### 互斥锁Lock

- 必须在全局定义，当作参数传给子进程
  
  - ```python
    from multiprocessing import Process, Lock
    
    def l(lock, num):
        lock.acquire()
        print("Hello Num: %s" % (num), flush=True)
        lock.release()
    
    if __name__ == '__main__':
        lock = Lock()  # 这个一定要定义为全局
        for num in range(20):
            Process(target=l, args=(lock, num)).start()
    ```

#### 信号量Semaphore

- 带计数功能的Lock
  
  - ```python
    from multiprocessing import Process, Semaphore
    import time, random
    
    def go_wc(sem, user):
        sem.acquire()
        print('%s 占到一个茅坑' % user, flush=True)
        time.sleep(random.randint(0, 3))
        sem.release()
        print(user, 'OK')
    
    if __name__ == '__main__':
        sem = Semaphore(2)
        p_l = []
        for i in range(5):
            p = Process(target=go_wc, args=(sem, 'user%s' % i,))
            p.start()
            p_l.append(p)
        for i in p_l:
            i.join()
    ```

## ProcessPoolExecutor

- 来源：`from concurrent.futures import ProcessPoolExecutor`
  
  - 相比multiprocessing封装更高层，可使用多进程和多线程且调用方式差不多
  
  - ```python
    import numpy as np
    import multiprocessing
    from concurrent.futures.process import ProcessPoolExecutor
    
    NUM_WORKERS = multiprocessing.cpu_count()
    np.random.seed(42)
    ARRAY_SIZE = int(2e8)
    data = np.random.random(ARRAY_SIZE)  # 任务：计算数组的sum
    
    def np_sum_global(start, stop):
        return np.sum(data[start:stop])
    
    def benchmark():
        chunk_size = int(ARRAY_SIZE / NUM_WORKERS)
        futures = []
        ts = time.time_ns()
        with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
            for i in range(0, NUM_WORKERS):
                start = i + chunk_size if i == 0 else 0
                futures.append(executor.submit(np_sum_global, start, i + chunk_size))
        futures, _ = concurrent.futures.wait(futures)
        return (time.time_ns() - start_time) / 1_000_000
    ```

- `map()`：批量执行任务
  
  - ```python
    def process_task(num):
        return num * num
    
    numbers = [1, 2, 3, 4, 5] 
    with ProcessPoolExecutor(max_workers=4) as executor:
        results = executor.map(process_task, numbers)
        for num, result in zip(numbers, results):
            print(f"{num} 的平方是 {result}")
    ```

- `submit()`：提交单个任务
  
  - ```python
    with ProcessPoolExecutor(max_workers=4) as executor:
        future = executor.submit(process_task, 10)  # 计算 10 的平方
        result = future.result()
        print(f"10 的平方是 {result}")
    ```
  
  - ```python
    from concurrent.futures import as_completed
    
    if __name__ == "__main__":
        numbers = [1, 2, 3, 4, 5]
        futures = []
    
        with ProcessPoolExecutor() as executor:
            # 批量提交任务
            for num in numbers:
                future = executor.submit(process_task, num)
                futures.append(future)
    
        # 处理已完成的任务（按完成顺序）
        for future in as_completed(futures):
            result = future.result()
            print(f"计算结果：{result}")
    ```

## 终止

- 默认情况：对主进程发送SIGINT（ctrl+c），主进程终止但子进程继续，子进程被系统托管（ppid=1）

- 修改：对主进程发送SIGINT（ctrl+c），主进程先终止子进程，然后再终结自己
  
  - ```python
    import signal
    
    def worker_task(worker_id):
        try:
            print(f"Worker {worker_id} 开始运行 (PID: {mp.current_process().pid})")
            while True:  # 模拟一个长时间运行的任务
                time.sleep(1)
                print(f"Worker {worker_id} 正在工作...")
        except KeyboardInterrupt:
            print(f"Worker {worker_id} 收到中断信号")
        except Exception as e:
            print(f"Worker {worker_id} 发生错误: {e}")
        finally:
            print(f"Worker {worker_id} 已退出")
    
    def handle_sigint(signum, frame, pool):  # 处理SIGINT信号的函数
        if pool:  # 终止所有子进程
            pool.terminate()
            print("所有子进程已被终止")
            pool.join()
            print("所有子进程已退出")
        sys.exit(0)  # 退出主进程
    
    def main():
        pool = mp.Pool(processes=5)
        # 为SIGINT信号注册处理函数，将进程池作为参数传递
        signal.signal(signal.SIGINT, lambda signum, frame: handle_sigint(signum, frame, pool))
        try:
            for i in range(5):
                pool.apply_async(worker_task, args=(i,))
            while True:
                time.sleep(1)
        except Exception as e:
            print(f"主进程发生错误: {e}")
        finally:  # 正常退出
            pool.close()
            pool.join()
    ```

## 多线程

- Python有GIL，同时只能有1个线程处于执行状态
  - **适合I/O或者爬虫，内存管理更方便，自带内存共享**
  - 由于GIL锁，不能用于计算密集型 => 应该用多进程

### 基于threading

- ```python
  import threading
  
  lock = threading.Lock()
  
  class Thread(threading.Thread):
      def __init__(self, thredID):
          threading.Thread.__init__(self)
          self.threadID = threadID
      def run(self)
          # calculate
          lock.acquire()
          # 修改临界区
          lock.release()
  
  if __name__ == '__main__':
      thread = []
      for i in range(thread_num):
          thread.append(Thread(i + 1))
      for i in range(thread_num):
          thread[i].start()
      for i in range(thread_num):
          thread[i].join()
  ```

## 专题

### 多进程多线程 I/O

- SSD 随机读取：大幅提升
  
  - SSD Query Depth (QD) 更大，更适合并行读写
  
  - QD：设备能同时处理的未完成I/O请求数

- SSD 顺序读取：提升，但不如SSD 随机读取明显
  
  - 系统会进行prefetch

- HDD 随机读取：提升
  
  - 系统会维护I/O队列，合并相邻读请求

- HDD 顺序读取：无提升

- 分析
  
  - 随机读取：每个进程/线程随机读取
    
    - 指标：IOPS，每秒完成的读写次数（4kB），评估小文件高并发场景（如数据库）
  
  - 顺序读取：每个进程/线程顺序读取
    
    - 指标：MB/s，评估大文件连续写
  
  - 若读入数据后统一操作，则多线程共享内存更方便；若读入后处理为计算密集型，则必须上多进程
