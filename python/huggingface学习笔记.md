# HuggingFaceå­¦ä¹ ç¬”è®°

- HuggingFaceï¼šå°†PLMå°è£…ä¸ºæ˜“è°ƒç”¨çš„æ¥å£

## pipeline

- æœ€é«˜çº§çš„å°è£…ï¼Œæ‰©å±•æ€§å·®
  
  - æä¾›äº†9ä¸ªå¸¸è§ä»»åŠ¡
  
  - ```python
    from transformers import pipeline
    
    # feature-extraction (get the vector representation of a text)
    # fill-mask
    # ner (named entity recognition)
    # question-answering
    # sentiment-analysis
    # summarization
    # text-generation
    # translation
    # zero-shot-classification
    
    classifier = pipeline("sentiment-analysis")  # æƒ…æ„Ÿåˆ†æï¼Œä¸‹è½½é»˜è®¤çš„æ¨¡å‹
    classifier(["I've been waiting for a HuggingFace course my whole life.",
                "I hate this so much!"])
    
    unmasker = pipeline("fill-mask")  # å¡«å…¥<mask>ï¼Œç»™å‡ºæœ€å¯èƒ½çš„top_kä¸ª
    unmasker("This course will teach you all about <mask> models.", top_k=2)  # å¡«ä¸Š<mask>ï¼Œç»™å‡ºæœ€å¯èƒ½çš„top_kä¸ª
    ```

- é¢„è®­ç»ƒå‚æ•°é»˜è®¤ä¿å­˜åœ¨`C:\users\xxx\.cache\huggingface`æˆ–`~/.cache/huggingface`

## Auto

### AutoTokenizer

- å°†å¥å­è½¬åŒ–ä¸ºtoken idåºåˆ—ï¼Œå¹¶åŠ å…¥ç‰¹æ®Štoken
  
  - ```python
    from transformers import AutoTokenizer
    
    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"  # æŒ‡å®šä¸‹è½½çš„æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    raw_inputs = ["I've been waiting for a HuggingFace course my whole life.",
                  "I hate this so much!"]
    # paddingï¼šå°†1ä¸ªbatchè¡¥å…¨è‡³ç›¸åŒé•¿åº¦
    # truncationï¼šå°†è¶…é•¿å¥å­é˜¶æ®µè‡³æœ€é•¿èƒ½æ¥å—é•¿åº¦
    # 'pt'ä»£è¡¨è¿”å›pytorch tensor
    inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors='pt')
    # inputs
    # è¿”å›çš„input_idsï¼štoken idåºåˆ—ï¼Œæœ‰è¡¥ä¸Š[CLS], [SEP]ç­‰ç‰¹æ®Štoken
    # è¿”å›çš„attention_maskï¼špaddingç»“æœ
    ```

- ä¸€æ­¥æ­¥æ‹†è§£
  
  - ```python
    tokens = [tokenizer.tokenize(sentence) for sentence in sentences]
    ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]  # ä¸è¡¥é›¶
    
    # ä¸€ä¸ªbatchå†…çš„è¾“å…¥é•¿åº¦åº”å½“ç›¸åŒï¼Œpadè‡³æœ€é•¿çš„ï¼Œtruncateè¶…è¿‡è¾“å…¥ä¸Šé™çš„
    # åº”åŒæ—¶è¾“å…¥attention maskï¼Œç»™å‡ºè¾“å…¥ä¸­å“ªäº›æ˜¯padding
    ouput = model(all_ids, attention_mask=attention_mask)
    ```

### AutoModel

- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºembedding
  
  - åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹**èˆå¼ƒäº†åªç”¨äºpre-trainçš„å‚æ•°ï¼Œä»…ä¿ç•™fine-tuneä¼šç”¨åˆ°çš„**
  
  - ä¸åŒ…å«åç»­åˆ†ç±»å¤´ï¼Œé€‚åˆç›´æ¥ç”¨äºinference
  
  - ```python
    from transformers import AutoModel
    
    model = AutoModel.from_pretrained(checkpoint)  # ä»HuggingFace Hubé€‰æ‹©model
    outputs = model(**inputs)
    print(outputs.last_hidden_state.shape)  # torch.tensorç±»å‹
    ```

### AutoModelForSequenceClassification

- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ¯”`AutoModel`å¤šå¼•å…¥äº†å¯è®­ç»ƒçš„åˆ†ç±»å¤´ï¼Œå¯è¾“å‡ºåˆ†ç±»å¤´çš„
  
  - é€‚åˆfine-tuneï¼Œåªéœ€åœ¨åé¢æ‰‹åŠ¨åŠ Softmaxå’Œäº¤å‰ç†µ
  
  - ```python
    from transformers import AdamW, AutoModelForSequenceClassification
    
    checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
    model = AutoModelForSequenceClassification.from_pretrained(checkpoint,
                                                               num_labels=2)
    outputs = model(**inputs)
    print(outputs.logits.shape)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    print(predictions)
    
    # è®­ç»ƒ
    optimizer = AdamW(model.parameters())
    loss = model(**inputs).loss
    loss.backward()
    optimizer.step()
    ```

## Config

- config: æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œå¦‚æ•°æ®é¢„å¤„ç†ã€ç½‘ç»œç»“æ„ã€è®­ç»ƒè¶…å‚æ•°
  
  - æ ¹æ®configèƒ½å”¯ä¸€åˆ›å»ºå‡º1ä¸ªæ¨¡å‹
  
  - ```python
    from transformers import BertConfig, BertModel
    
    bert_config = BertConfig.from_pretrained('xxx')  # å¯è‡ªé€‰å“ªä¸ªcheck_point
    bert_config = BertConfig.from_pretrained('xxx', num_hidden_layers=10)  # å¯æ”¹ç»“æ„Â·
    bert_model = BertModel(bert_config)
    bert_model.save_pretrained(pth)  # ä¿å­˜ä¸º2ä¸ªæ–‡ä»¶ï¼šconfig.json pytorch_model.bin
    
    bert_model.from_pretrained(local_pth)  # è¯»å…¥å­˜åœ¨æœ¬åœ°çš„æ¨¡å‹
    ```

- ä½¿ç”¨æ¨¡å‹
  
  - ```python
    import torch
    
    sequences = ["Hello!", "Cool.", "Nice!"]
    encoded_sequences = [[101, 7592, 999, 102],
                         [101, 4658, 1012, 102],
                         [101, 3835, 999, 102],]
    model_inputs = torch.tensor(encoder_sequences)
    output = model(model_inputs)
    ```

## datasets

- HuggingFace Hubæä¾›äº†å¾ˆå¤šè®­ç»ƒæ•°æ®é›†
  
  - å¯ç›´æ¥ä½¿ç”¨å°è£…çš„`datsets`ç±»ä¸‹è½½å¹¶å¤„ç†
  
  - ```python
    from datasets import load_dataset
    
    raw_datasets = load_dataset('glue', 'mrpc')  # è¿”å›dict
    # raw_datasets['train'], raw_datasets.features, raw_datasets[0]
    ```
  
  - æ•°æ®é›†ä¿å­˜åœ¨diskä¸­ï¼Œä»…éœ€è¦æ—¶æ‰è¯»å…¥å†…å­˜

- æ•°æ®é›†é¢„å¤„ç†ï¼šä¾‹å¦‚tokenize
  
  - ```python
    def tokenize_func(example):
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
        # è¿”å›input_ids, attention_mask, and token_type_ids
    
    tokenized_datasets = load_dataset('glue', 'mrpc').map(tokenize_func, batched=True)
    # batched=Trueä½¿ç”¨å¹¶è¡Œå¤„ç†
    # mapæ˜¯å¯¹æ•´ä¸ªæ•°æ®é›†é¢„å¤„ç†
    
    # ä¹‹åç›´æ¥ç”¨**batchè¾“å…¥æ¨¡å‹ï¼Œæ‰€ä»¥è¦å»æ‰æ‰€æœ‰ä¸èƒ½è¾“å…¥æ¨¡å‹çš„
    tokenized_datasets = tokenized_datasets.remove_column(['idx', 'sentence1', 'sentence2'])
    # pytorchåªè®¤keyä¸ºlabelsçš„ä¸ºæ ‡ç­¾ï¼Œæ•…éœ€è°ƒæ•´
    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')
    tokenized_datasets.set_format('torch')
    
    # ä¹‹åå¯ç›´æ¥ä½¿ç”¨DataLoader(tokenized_datasets)
    ```

- Dynamic Padding
  
  - åœ¨é¢„å¤„ç†å‡½æ•°ä¸­padæ…¢
    
    - éœ€è¦å›ºå®šé•¿åº¦çš„batchï¼ˆä¾‹å¦‚TPUï¼‰ï¼Œåˆ™ä»éœ€è¦åœ¨é¢„å¤„ç†ä¸­åš
  
  - æ›´å¿«ï¼šå…ˆç”Ÿæˆbatchï¼Œå†åœ¨DataLoaderçš„collect_fnå†™pad
  
  - ```python
    from torch.utils.data import DataLoader
    from transformers import DataCollatorWithPadding
    
    data_collator = DataCollatorWithPadding(tokenizer)
    train_dataloader = DataLoader(tokenized_datasets['train'],
                                  batch_size=16, shuffle=True,
                                  collate_fn=data_collator)  # åœ¨DataLoaderæ‰åšpad
    ```

## Trainer

```python
from transformers import Trainer, TrainingArguments
from datasets import load_metric

# é»˜è®¤ä½¿ç”¨å…¨éƒ¨å¯è§GPU
# os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"

training_args = TrainingArguments('ä¿å­˜è®­ç»ƒè¶…å‚æ•°å’Œcheckpointçš„åœ°å€')  # è®¾ç½®è¶…å‚æ•°ç­‰
# per_device_train_batch_size, per_device_eval_batch_size
# per_device_train_batch_size, per_device_eval_batch_size, é»˜è®¤ddp
# num_train_epochs, learning_rate, weight_decay
# evaluation_strategy='epoch' or 'steps'
# seed

# å›ºå®šä¸»å¹²ç½‘çš„å‚æ•°ï¼Œä»…è°ƒclassifier
for param in model.bert.parameters():  # è°ƒè¯•æ¨¡å¼ä¸‹æŸ¥çœ‹ç½‘ç»œæ¶æ„ï¼Œåç«¯çš„åˆ†ç±»ç½‘ç»œæ˜¯model.classifier
    param.requires_grad = False

# éªŒè¯é›†
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# è®­ç»ƒ
trainer = Trainer(model, training_args,
                  training_dataset=tokenized_datasets['train'],
                  eval_dataset=token_datasets['validation'],
                  data_collator=data_collator,
                  tokenizer=tokenizer,
                  compute_metrics=compute_metrics,)
trainer.train()

# é¢„æµ‹
predictions = trainer.predict(tokenizerd_datasets['validation'])
# åŒ…å«3ä¸ªå…ƒç´ : predictions (logits), label_ids (ground truth), metrics
preds = np.argmax(predictions.predictions, axis=-1)
metric = load_metric('glue', 'mrpc')
metric.compute(preds, predictions.label_ids)
```

- Traineræ˜¯éå¸¸é¡¶å±‚çš„å°è£…ï¼Œä¸pytorch lightningç›¸ä¼¼
  - é€‚åˆå†™å¥½äº†åœ¨å¤šæœºå¤šå¡ä¸Šè¿è¡Œï¼Œå¯¹ddpå°è£…è¾ƒå¥½
  - ä¸é€‚åˆä¸æ–­æ”¹è¿›ï¼Œè¿™ç§é€‚åˆç”¨pytorchå†™forå¾ªç¯

## evaluate

- evaluateåº“ï¼šè®¡ç®—åº¦é‡æ ‡æ³¨ï¼Œéœ€è¦è‡ªå·±å®ç°é¢„æµ‹logits

## å®Œæ•´è®­ç»ƒä»£ç 

```python
import evaluate
import argparse
import torch
import numpy as np

from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import AutoTokenizer, AdamW, get_scheduler
from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification


class Classifier:
    def __init__(self, opt) -> None:
        self.opt = opt
        self.device = torch.device(f'cuda:{opt.card_id}')

        MODEL = 'cardiffnlp/twitter-roberta-base-sentiment'
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)
        # åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼Œnum_labelsæŒ‡å®šåˆ†ç±»å¤´å°ºå¯¸
        self.model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=5,
                                                                        ignore_mismatched_sizes=True)
        self.model.to(self.device)
        raw_dataset = load_dataset('SetFit/sst5')  # æƒ…æ„Ÿåˆ†æï¼Œ5åˆ†ç±»
        self.tokenized_datasets = raw_dataset.map(self.tokenize, batched=True)
        self.tokenized_datasets = self.tokenized_datasets.remove_columns(['text', 'label_text'])
        self.tokenized_datasets = self.tokenized_datasets.rename_column('label', 'labels')
        self.tokenized_datasets.set_format('torch')

        self.data_collator = DataCollatorWithPadding(self.tokenizer)
        self.train_dataloader = DataLoader(self.tokenized_datasets['train'],
                                           batch_size=opt.bs, shuffle=True,
                                           collate_fn=self.data_collator)  # åœ¨DataLoaderæ‰åšpad
        self.validation_dataloader = DataLoader(self.tokenized_datasets['validation'],
                                                batch_size=opt.bs, shuffle=False,
                                                collate_fn=self.data_collator)
        self.test_dataloader = DataLoader(self.tokenized_datasets['test'],
                                          batch_size=opt.bs, shuffle=False,
                                          collate_fn=self.data_collator)

        self.optimizer = AdamW(self.model.parameters())
        for param in self.model.roberta.parameters():  # ä»…è°ƒæœ€åçš„åˆ†ç±»å¤´
            param.requires_grad = False
        num_training_steps = self.opt.epoch * len(self.train_dataloader)
        # å­¦ä¹ ç‡åœ¨num_warmup_stepsæ­¥å†…ä»0çº¿æ€§å¢é•¿åˆ°å®šä¹‰optimæ—¶çš„lrï¼Œå†çº¿æ€§é€’å‡åˆ°0
        self.lr_scheduler = get_scheduler(name='linear', optimizer=self.optimizer,
                                          num_warmup_steps=300, num_training_steps=num_training_steps)

    def tokenize(self, data):
        return self.tokenizer(data['text'], truncation=True, max_length=512)

    def train(self):
        best_acc, best_model = 0, None
        for ep in range(self.opt.epoch):
            self.model.train()
            for batch in self.train_dataloader:
                batch = {k: v.to(self.device) for k, v in batch.items()}
                loss = self.model(**batch).loss  # å­—å…¸çš„keyä½œä¸ºå‚æ•°åï¼Œvalueä½œä¸ºå‚æ•°å€¼
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                self.lr_scheduler.step()

            result = self.validation()
            if best_acc < result['accuracy']:
                best_acc = result['accuracy']
                best_model = copy.deepcopy(self.model.state_dict())
            print(f"epoch {ep} validation: [accuracy: {result['accuracy']}]")
        self.model.load_state_dict(best_model)
        result = self.test()
        print(f"test result: [accuracy: {result['accuracy']}]")

    def validation(self):
        metric = evaluate.load('accuracy')
        self.model.eval()
        for batch in self.validation_dataloader:
            batch = {k: v.to(self.device) for k, v in batch.items()}
            with torch.no_grad():
                outputs = self.model(**batch)
                logits = outputs.logits
                predictions = torch.argmax(logits, dim=-1)
                metric.add_batch(predictions=predictions, references=batch["labels"])
        result = metric.compute()
        return result

    def test(self):
        metric = evaluate.load('accuracy')
        self.model.eval()
        for batch in self.test_dataloader:
            batch = {k: v.to(self.device) for k, v in batch.items()}
            with torch.no_grad():
                outputs = self.model(**batch)
                logits = outputs.logits
                predictions = torch.argmax(logits, dim=-1)
                metric.add_batch(predictions=predictions, references=batch["labels"])
        result = metric.compute()
        return result


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--epoch', default=20)
    parser.add_argument('--bs', default=32)
    parser.add_argument('--lr', default=1e-5)
    parser.add_argument('--card_id', default=2)
    opt = parser.parse_args()

    C = Classifier(opt)
    C.train()
```

## Adapter

```python
# ä½¿ç”¨é¢„è®­ç»ƒRoBERTaï¼Œåœ¨çƒ‚ç•ªèŒ„æ•°æ®ä¸Šå¾®è°ƒæƒ…æ„Ÿåˆ†æä»»åŠ¡
import numpy as np
from datasets import load_dataset
from transformers import RobertaTokenizer
from transformers import RobertaConfig, RobertaModelWithHeads
from transformers import TrainingArguments, AdapterTrainer, EvalPrediction

dataset = load_dataset("rotten_tomatoes")

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def encode_batch(batch):
  """Encodes a batch of input data using the model tokenizer."""
  return tokenizer(batch["text"], max_length=80, truncation=True, padding="max_length")

# Encode the input data
dataset = dataset.map(encode_batch, batched=True)
# The transformers model expects the target class column to be named "labels"
dataset.rename_column_("label", "labels")
# Transform to pytorch tensors and only output the required columns
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

config = RobertaConfig.from_pretrained("roberta-base", num_labels=2)
model = RobertaModelWithHeads.from_pretrained("roberta-base", config=config)

# Add a new adapter
model.add_adapter("rotten_tomatoes")
# Add a matching classification head
model.add_classification_head("rotten_tomatoes", num_labels=2, id2label={ 0: "ğŸ‘", 1: "ğŸ‘"})
# Activate the adapter
model.train_adapter("rotten_tomatoes")

training_args = TrainingArguments(
    learning_rate=1e-4,
    num_train_epochs=6,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    logging_steps=200,
    output_dir="./training_output",
    overwrite_output_dir=True,
    # The next line is important to ensure the dataset labels are properly passed to the model
    remove_unused_columns=False,
)

def compute_accuracy(p: EvalPrediction):
  preds = np.argmax(p.predictions, axis=1)
  return {"acc": (preds == p.label_ids).mean()}

trainer = AdapterTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    compute_metrics=compute_accuracy)

trainer.train()
trainer.evaluate()
```

## ç§‘å­¦ä¸Šç½‘

- ç›®å‰å›½å†…æ— æ³•ç›´è¿åˆ°huggingface

- ä¸‹è½½huggingfaceé¢„è®­ç»ƒæ¨¡å‹
  
  - æŒ‚æ¢¯å­ï¼Œä»huggingfaceä¸‹è½½æ–‡ä»¶
  - é•œåƒç«™ï¼š`hf-mirror.com`
    - `export HF_ENDPOINT=https://hf-mirror.com`ï¼Œå†è°ƒç”¨ä¸‹è½½è„šæœ¬
    - `os.environ['HF-ENDPOINT'] = 'https://hf-mirror.com'`
  - ä»å›½å†…é•œåƒç«™æ‰¾ï¼ˆæ¨¡å‹å°‘ï¼‰ï¼š`aliendao.cn`, `modelscope.cn`

- åŠ è½½æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹
  
  - æ³•1ï¼šå°†æ¨¡å‹åæ›´æ¢ä¸ºä¸‹è½½å¥½çš„æ–‡ä»¶å¤¹
  - æ³•2ï¼šæ‰‹åŠ¨æ„å»º`.cache`
    - ä»å®˜ç½‘ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œé»˜è®¤å­˜åœ¨`~/.cache/huggingface/hub/`ä¸‹é¢

- é¢„è®­ç»ƒæ¨¡å‹åœ¨`.cache`çš„å­˜å‚¨å½¢å¼
  
  - æ–‡ä»¶å¤¹åç§°ï¼š`models--{ä¸Šä¼ ç”¨æˆ·å}--{æ¨¡å‹å}`
    - å†…éƒ¨åŒ…å«`refs, blobs, snapshots`ä¸‰ä¸ªæ–‡ä»¶å¤¹
  - `refs`ï¼šå­˜æ”¾æ¯æ¬¡commitçš„hash
    - ä¾‹ï¼šä»…æœ‰1ä¸ªmainæ–‡ä»¶ï¼ˆbranchåç§°ï¼‰ï¼Œå†…å®¹æ˜¯æœ€æ–°çš„commitçš„hashï¼ˆæ²¡æœ‰`\n`ï¼‰
      - åˆ é™¤\nï¼š`tr -d "\n" < refs/main_cp > refs/main`
  - `blobs`ï¼šå­˜æ”¾å®é™…ä¸‹è½½çš„æ–‡ä»¶ï¼Œå¦‚config.jsonå’Œæ¨¡å‹å‚æ•°
    - ä»`snapshots`ä¸­çš„æ–‡ä»¶hard linkè¿‡æ¥
    - è‹¥ä¸ºconfigæ–‡ä»¶ï¼Œåˆ™æ–‡ä»¶åæ”¹ä¸ºå…¶git hash
      - `git hash-object $file`
    - è‹¥ä¸ºæ¨¡å‹å‚æ•°æ–‡ä»¶ (LFS)ï¼Œåˆ™æ–‡ä»¶åæ”¹ä¸ºå…¶sha256 hash
      - `openssl sha256 $file`
    - æ ¹æ®hashå€¼ï¼Œåˆ¤æ–­ä¸åŒcommitæ˜¯å¦ä¿®æ”¹äº†æ¯ä¸ªæ–‡ä»¶ï¼Œè‹¥æœªä¿®æ”¹ï¼Œåˆ™ä¸ç”¨ä¸‹è½½
  - `snapshots`ï¼šå­˜æ”¾æ¯æ¬¡commitçš„æ–‡ä»¶çš„è½¯é“¾æ¥ï¼ˆå®é™…æ˜¯ç›´æ¥æ‹·è´ï¼‰
    - å­æ–‡ä»¶å¤¹åï¼šç›¸åº”commitçš„hash
    - å­æ–‡ä»¶å¤¹å†…å®¹ï¼šç›¸åº”commitçš„æ–‡ä»¶ï¼Œä¾‹å¦‚config.json, pytorch_model.bin
    - ä¸å®˜ç½‘çš„ä¸€è‡´ï¼Œä¸éœ€è¦ä¿®æ”¹åç§°
